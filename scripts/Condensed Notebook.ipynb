{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1019c2",
   "metadata": {},
   "source": [
    "In this notebook, I aim to roll through an analysis across a few patients which can easily be extended for all of the  patients in your cohort. To do so, we will use the pre-processing functions that are written out more explicitly in the step-by-step notebooks. \n",
    "\n",
    "**This is the notebook you should copy and edit for your own actual analyses**\n",
    "\n",
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5720f96",
   "metadata": {},
   "source": [
    "These are magics that provide certain functionality. Specifically, if you edit functions that are called in this notebook, the functions are reloaded so the changes propagate here without needing to reload the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b857e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903fdada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress, ttest_ind, ttest_rel, ttest_1samp\n",
    "import pandas as pd\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "import os \n",
    "import joblib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import warnings \n",
    "\n",
    "# I only want to see warnings once\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd439a",
   "metadata": {},
   "source": [
    "Note: If you have installed the LFPAnalysis package in editable form on Minerva, you must append the local path! This is because Minerva requires that you point your package installs away from the local directory for space reasons, but editable packages have to be installed locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3889b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/hpc/users/qasims01/resources/LFPAnalysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b549c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LFPAnalysis import lfp_preprocess_utils, sync_utils, analysis_utils, nlx_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae1c39",
   "metadata": {},
   "source": [
    "## IF USING YOUR OWN DATA: Pre-process (run 1x): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0c76f",
   "metadata": {},
   "source": [
    "In the pre-processing functions below, we: \n",
    "\n",
    "1. load the raw data (either a .edf file or a folder of .nlx files) into mne objects for use with the mne toolbox: https://mne.tools/stable/index.html.\n",
    "\n",
    "2. load the localized electrode names from the .csv or .xlsx file listing their MNI coordinates into the mne object\n",
    "\n",
    "3. filter and resample as necessary\n",
    "\n",
    "4. re-reference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774ad4e-52ba-43b7-83f9-328af06eab32",
   "metadata": {},
   "source": [
    "**NOTE**: this notebook is meant to use the sample data, stored locally in the repo. This local data was pre-processed using the follow cell, which you will need to uncomment and modify according to your own data folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83f681a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for ix, subj_id in enumerate(subj_ids): \n",
    "#     site = subj_sites[ix]\n",
    "#     format = subj_formats[ix]\n",
    "    \n",
    "#     print(f'Working on subj {subj_id}')\n",
    "    \n",
    "#     # Set paths\n",
    "#     load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "#     elec_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/anat/'\n",
    "#     save_path = f'{base_dir}/projects/guLab/Salman/EphysAnalyses/{subj_id}/neural/Day1'\n",
    "    \n",
    "#     # Check if path exists for saving, and if not, make it\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "\n",
    "#     # electrode files could either be csv or excel\n",
    "#     elec_files = glob(f'{elec_path}/*.csv') + glob(f'{elec_path}/*.xlsx')\n",
    "#     # There should really only be one, so grab it with the zero-index \n",
    "#     elec_file = elec_files[0]\n",
    "\n",
    "#     # Make MNE file\n",
    "#     mne_data = lfp_preprocess_utils.make_mne(load_path=load_path, \n",
    "#                                              elec_path=elec_file,\n",
    "#                                              format=format,\n",
    "#                                              return_data=True,\n",
    "#                                              site=site,\n",
    "#                                              check_bad=False) # changed this to not annotate anything as bad \n",
    "\n",
    "#     # Save this data so that you don't need this step again:\n",
    "#     mne_data.save(f'{save_path}/raw_ieeg.fif', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835543d",
   "metadata": {},
   "source": [
    "## Re-reference the data (default=bipolar): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f462f2e-2a28-47c3-bce4-6e064ec4f1c0",
   "metadata": {},
   "source": [
    "We re-reference the data to get rid of shared noise, cleaning the data to leave what we assume is local biological activity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3958acf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /hpc/users/qasims01/resources/LFPAnalysis/data/sample_ieeg.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 394061 =      0.000 ...   788.122 secs\n",
      "Ready.\n",
      "Reading 0 ... 394061  =      0.000 ...   788.122 secs...\n",
      "sEEG channel type selected for re-referencing\n",
      "Creating RawArray with float64 data, n_channels=15, n_times=394062\n",
      "    Range : 0 ... 394061 =      0.000 ...   788.122 secs\n",
      "Ready.\n",
      "Added the following bipolar channels:\n",
      "racas1-racas2, racas2-racas3, racas3-racas4, racas4-racas5, racas5-racas6, racas6-racas7, racas8-racas9, racas9-racas10, rmolf1-rmolf2, rmolf2-rmolf3, rmolf3-rmolf4, rmolf4-rmolf5, rmolf5-rmolf6, rmolf9-rmolf10, rmolf10-rmolf11\n",
      "Writing /hpc/users/qasims01/resources/LFPAnalysis/data/sample_ieeg_bp.fif\n",
      "Closing /hpc/users/qasims01/resources/LFPAnalysis/data/sample_ieeg_bp.fif\n",
      "[done]\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n",
      "/hpc/users/qasims01/resources/LFPAnalysis/LFPAnalysis/../data/YBA_ROI_labelled.xlsx\n"
     ]
    }
   ],
   "source": [
    "# for ix, subj_id in enumerate(subj_ids): \n",
    "site = 'MSSM'\n",
    "format = 'edf'\n",
    "    \n",
    "# Set load path\n",
    "load_path = '/hpc/users/qasims01/resources/LFPAnalysis/data'\n",
    "\n",
    "# Load electrode file \n",
    "elec_file = f'{load_path}/sample_labels.xlsx'\n",
    "\n",
    "# Make MNE file\n",
    "mne_data = mne.io.read_raw_fif(f'{load_path}/sample_ieeg.fif', preload=True)\n",
    "\n",
    "\n",
    "# Re-reference neural data\n",
    "mne_data_reref = lfp_preprocess_utils.ref_mne(mne_data=mne_data, \n",
    "                                              elec_path=elec_file, \n",
    "                                              method='bipolar', \n",
    "                                              site=site)\n",
    "\n",
    "# Save this data so that you don't need this step again:\n",
    "mne_data_reref.save(f'{load_path}/sample_ieeg_bp.fif', overwrite=True)\n",
    "\n",
    "# Should also save out re-referenced elec_file: \n",
    "\n",
    "elec_data = lfp_preprocess_utils.load_elec(elec_file)\n",
    "anode_list = [x.split('-')[0] for x in mne_data_reref.ch_names]\n",
    "elec_df = elec_data[elec_data.label.str.lower().isin(anode_list)]\n",
    "elec_df['label'] =  elec_df.label.apply(lambda x: [a for a in mne_data_reref.ch_names if str(x).lower() in a.split('-')[0]][0])\n",
    "\n",
    "# Add region to the data frame \n",
    "\n",
    "manual_col = [col for col in elec_df.columns if 'manual' in col.lower()][0]\n",
    "all_regions = [] \n",
    "for chan_name in elec_df.label.unique():\n",
    "    elec_region = analysis_utils.select_rois_picks(elec_df, chan_name, manual_col=manual_col)\n",
    "    all_regions.append(elec_region) \n",
    "\n",
    "elec_df['salman_region'] = all_regions\n",
    "elec_df['hemisphere'] = elec_df.label.apply(lambda x: x[0])\n",
    "\n",
    "elec_df.to_csv(f'{load_path}/sample_labels_bp', index=False)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff05564",
   "metadata": {},
   "source": [
    " - mne_data: a Raw mne object, where the data has been loaded, filtered for line noise, parsed for different data types, and resampled if necessary. \n",
    " \n",
    " - mne_data_reref: an mne object containing re-referenced data (either white matter or bipolar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b53a3d",
   "metadata": {},
   "source": [
    "## NOW look at the data to manually remove channels: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b12839",
   "metadata": {},
   "source": [
    "After bipolar referencing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll up/down and left/right using your keyboard. CLICK on a channel to turn it 'grey' and mark as a 'bad' channel. \n",
    "# If you click a grey channel again it will unmark it. \n",
    "\n",
    "# subj_id = 'MS012'\n",
    "# save_path = f'{base_dir}/projects/guLab/Salman/EphysAnalyses/{subj_id}/neural/Day1'\n",
    "mne_data_reref = mne.io.read_raw_fif(f'{load_path}/sample_ieeg_bp.fif', preload=True)\n",
    "fig = mne_data_reref.plot(start=0, duration=120, n_channels=30, \n",
    "                      scalings=mne_data_reref._data.max()/30\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90817f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALSO look at the power spectra! \n",
    "# You can click on channels here to identify them, and go back to the viz above to mark them as noise if need be\n",
    "\n",
    "mne_data_reref.compute_psd().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d310de9",
   "metadata": {},
   "source": [
    "If you have ran the preprocessing above, load the data instead: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b657ca72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /hpc/users/qasims01/resources/LFPAnalysis/data/sample_ieeg_bp.fif...\n",
      "    Range : 0 ... 394061 =      0.000 ...   788.122 secs\n",
      "Ready.\n",
      "Reading 0 ... 394061  =      0.000 ...   788.122 secs...\n",
      "Opening raw data file /hpc/users/qasims01/resources/LFPAnalysis/data/sample_photodiode.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 807039 =      0.000 ...   788.124 secs\n",
      "Ready.\n",
      "Reading 0 ... 807039  =      0.000 ...   788.124 secs...\n"
     ]
    }
   ],
   "source": [
    "# for subj_id in subj_ids: \n",
    "load_path = '/hpc/users/qasims01/resources/LFPAnalysis/data'\n",
    "\n",
    "elec_file = f'{load_path}/sample_labels.xlsx'\n",
    "\n",
    "elec_data = lfp_preprocess_utils.load_elec(elec_file)\n",
    "\n",
    "mne_data_reref = mne.io.read_raw_fif(f'{load_path}/sample_ieeg_bp.fif', preload=True)\n",
    "\n",
    "photodiode_data = mne.io.read_raw_fif(f'{load_path}/sample_photodiode.fif', preload=True)\n",
    "\n",
    "# Append to list \n",
    "# mne_dict[subj_id].append(mne_data_reref)\n",
    "\n",
    "# photodiode_dict[subj_id].append(photodiode_data)\n",
    "\n",
    "# elec_dict[subj_id].append(elec_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e10c5",
   "metadata": {},
   "source": [
    " - mne_dict: a dictionary containing all of your subjects' re-referenced mne data \n",
    " \n",
    " - photodiode_dict: a dictionary containing all of your subjects' photodiode data \n",
    " \n",
    " - elec_dict: a dictionary containing the paths to your subjects' electrode data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b7377",
   "metadata": {},
   "source": [
    "## Sync behavioral and neural data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57df8f",
   "metadata": {},
   "source": [
    "Here, we perform a critical step: computing the time offset between the computer that recorded the neural data and the laptop that featured the experiment. \n",
    "\n",
    "The function here only requires a **subset** of detected sync signals (i.e. photodiode deflections) to be detected to successfully compute this offset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501a02c-8417-4608-b8ab-dcefe44c53f8",
   "metadata": {},
   "source": [
    "First, you may need to MANUALLY clean the photodiode signal if the recording quality is poor. Load it, plot it, and try to isolate/amplify the pulses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d6565-fd46-43fe-9266-68e7f05c1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_id = 'MS015'\n",
    "# temp_diode = photodiode_dict[subj_id][0]._data[0, :]\n",
    "# temp_diode[900000:] = np.nanmin(temp_diode)\n",
    "# photodiode_dict[subj_id][0]._data = temp_diod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480fb71d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 blocks\n",
      "............................\n",
      "\n",
      "found matches for 150 of 428 pulses\n",
      "0.9999892560991055\n",
      "-228.2080128605071\n"
     ]
    }
   ],
   "source": [
    "# slopes = {f'{x}': [] for x in subj_ids}\n",
    "# offsets = {f'{x}': [] for x in subj_ids}\n",
    "\n",
    "# for subj_id in subj_ids: \n",
    "        \n",
    "# Load the behavioral timestamps: \n",
    "#     behav_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/behav/Day1'\n",
    "time_df = pd.read_csv(f'{load_path}/sample_ts.csv')\n",
    "# Load in the timestamps pertaining to your sync. If your task had a square pop up, for example, grab the times for that square's appearance from the behavioral logs.\n",
    "# Below, I do this for my own task's Psychopy output, but yours is probably coded differently. \n",
    "# beh_ts = temp_df[temp_df.keys()[temp_df.keys().str.startswith('sync') & temp_df.keys().str.endswith('started')]].values\n",
    "# beh_ts = beh_ts[~np.isnan(beh_ts)] \n",
    "\n",
    "# Synchronize to the photodiode or whatever your neural sync signal is\n",
    "height = 1\n",
    "windSize = 15\n",
    "smoothSize = 11\n",
    "\n",
    "slope, offset = sync_utils.synchronize_data(time_df['beh_ts'].values, \n",
    "                                            photodiode_data, \n",
    "                                            smoothSize=smoothSize, windSize=windSize, height=height)\n",
    "\n",
    "print(slope)\n",
    "print(offset)\n",
    "# slopes[subj_id].append(slope)\n",
    "# offsets[subj_id].append(offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcdc1d",
   "metadata": {},
   "source": [
    " - slopes: a dictionary containing the slopes (should be ~ 1) for each subject\n",
    " - offsets: a dictionary containing the offsets for each subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd052688",
   "metadata": {},
   "source": [
    "## Load your behavioral data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1451c",
   "metadata": {},
   "source": [
    "You probably have a separate notebook for processing the behavioral data for your task. Load the processed dataframe here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4de3ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "behav_data = pd.read_csv(f'{load_path}/sample_beh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9e1bc09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trials</th>\n",
       "      <th>feedback_start</th>\n",
       "      <th>baseline_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>243.239158</td>\n",
       "      <td>244.929025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>248.344187</td>\n",
       "      <td>250.043187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>254.083059</td>\n",
       "      <td>255.790670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>258.148220</td>\n",
       "      <td>259.838892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>261.943712</td>\n",
       "      <td>263.620631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trials  feedback_start  baseline_start\n",
       "0       1      243.239158      244.929025\n",
       "1       2      248.344187      250.043187\n",
       "2       3      254.083059      255.790670\n",
       "3       4      258.148220      259.838892\n",
       "4       5      261.943712      263.620631"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behav_data.head(5)[['trials', 'feedback_start', 'baseline_start']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833f764",
   "metadata": {},
   "source": [
    "## Make epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944dc32",
   "metadata": {},
   "source": [
    "Make epochs and remove IEDs. Currently just doing this for one example period - when subjects receive feedback. \n",
    "\n",
    "Notes: \n",
    "\n",
    "- I also segment a baseline period for every event of interest. \n",
    "\n",
    "- I apply a buffer period of 1.0 seconds - this will be helpful when we compute spectrograms later. \n",
    "\n",
    "- The IED count for every channel is added to the epoch metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65b21a",
   "metadata": {},
   "source": [
    "(I'm a little dumb, so my baseline is a fixation cross AFTER the trial, rather than before. A bit silly if you ask me.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a53422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /hpc/users/qasims01/resources/LFPAnalysis/data/sample_ieeg_bp.fif...\n",
      "    Range : 0 ... 394061 =      0.000 ...   788.122 secs\n",
      "Ready.\n",
      "Reading 0 ... 394061  =      0.000 ...   788.122 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 25 - 80 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 25.00\n",
      "- Lower transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 21.88 Hz)\n",
      "- Upper passband edge: 80.00 Hz\n",
      "- Upper transition bandwidth: 20.00 Hz (-6 dB cutoff frequency: 90.00 Hz)\n",
      "- Filter length: 265 samples (0.530 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    9.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feedback_start']\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 2001 original time points ...\n",
      "0 bad epochs dropped\n",
      "Opening raw data file /hpc/users/qasims01/resources/LFPAnalysis/data/sample_ieeg_bp.fif...\n",
      "    Range : 0 ... 394061 =      0.000 ...   788.122 secs\n",
      "Ready.\n",
      "Reading 0 ... 394061  =      0.000 ...   788.122 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 25 - 80 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 25.00\n",
      "- Lower transition bandwidth: 6.25 Hz (-6 dB cutoff frequency: 21.88 Hz)\n",
      "- Upper passband edge: 80.00 Hz\n",
      "- Upper transition bandwidth: 20.00 Hz (-6 dB cutoff frequency: 90.00 Hz)\n",
      "- Filter length: 265 samples (0.530 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['baseline_start']\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1376 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "# set some windows of interest \n",
    "\n",
    "buf = 1.0 # this is the buffer before and after that we use to limit edge effects for TFRs\n",
    "\n",
    "IED_args = {'peak_thresh':5,\n",
    "           'closeness_thresh':0.25, \n",
    "           'width_thresh':0.2}\n",
    "\n",
    "# evs = ['gamble_start', 'feedback_start', 'baseline_start']\n",
    "evs = {'feedback_start': [-0.5, 1.5],\n",
    "       'baseline_start': [0, 0.75]}\n",
    "\n",
    "load_path = '/hpc/users/qasims01/resources/LFPAnalysis/data'\n",
    "\n",
    "# add behavioral times of interest \n",
    "# for subj_id in subj_ids:\n",
    "    # Set paths\n",
    "#     load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "#     save_path = f'{base_dir}/projects/guLab/Salman/EphysAnalyses/{subj_id}/neural/Day1'\n",
    "\n",
    "epochs_all_evs = {f'{x}': np.nan for x in evs}\n",
    "for event in evs.keys():\n",
    "    pre = evs[event][0]\n",
    "    post = evs[event][1]\n",
    "    fixed_baseline = None\n",
    "#     behav_times = learn_df[(learn_df.participant==subj_id)][event]\n",
    "    behav_times = behav_data[event]\n",
    "\n",
    "    # THE following function will now SAVE out dataframes that indicate IED and artifact time points in your data\n",
    "\n",
    "    epochs = lfp_preprocess_utils.make_epochs(load_path=f'{load_path}/sample_ieeg_bp.fif', \n",
    "                                              slope=slope, offset=offset, \n",
    "                                              behav_name=event, behav_times=behav_times,\n",
    "                                              ev_start_s=pre, ev_end_s=post, buf_s=1, downsamp_factor=None, IED_args=IED_args, detrend=0)\n",
    "\n",
    "\n",
    "    epochs_all_evs[event] = epochs\n",
    "    epochs_all_evs[event].save(f'{load_path}/sample_{event}-epo.fif', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0abb83a",
   "metadata": {},
   "source": [
    " - epochs_all_evs: dictionary containing the epochs for all of your subjects' re-referenced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7661bf",
   "metadata": {},
   "source": [
    "Plot and examine the epochs if you'd like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af96abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# fig = epochs_all_subjs_all_evs['MS007']['feedback_start'].plot(n_epochs=10, n_channels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6261e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need this following line to save the annotations to the epochs object \n",
    "# fig.fake_keypress('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af3e03",
   "metadata": {},
   "source": [
    "## Where do I go from here? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5654949-ac77-4f95-b8bd-20451b63b8c6",
   "metadata": {},
   "source": [
    "At this point, you've successfuly pre-processed your iEEG data and sliced it around your timepoints of interest. These epochs are going to be the currency for many of your subsequent analyses, so make sure you TRUST THEM before proceeding to the other notebooks for analyses. \n",
    "\n",
    "From here, you can move on to the:\n",
    "\n",
    "1. FOOOF: a notebook for computing power-spectra across trials and fitting their peaks \n",
    "\n",
    "2. TFRPlotsAndStatistics: a notebook for computing time-frequency spectra (trial-level), and computing several different statistics or simply saving the data out in dataframes. \n",
    "\n",
    "3. OscillationDetection(BOSC): a notebook for computing sliding burst detection and saving the data out in dataframes \n",
    "\n",
    "4. TimeResolvedRegression: a notebook for computing regression analysis at each timepoint of a timeseries. TFR-extracted band power is used as example. \n",
    "\n",
    "5. ConnectivityAnalysis: a notebook for computing different synchrony measures between electrodes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfpanalysis",
   "language": "python",
   "name": "lfpanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
