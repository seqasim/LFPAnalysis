{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1019c2",
   "metadata": {},
   "source": [
    "In this notebook, I aim to roll through an analysis across a few patients which can easily be extended for all of the  patients in your cohort. To do so, we will use the pre-processing functions that are written out more explicitly in the step-by-step notebooks. \n",
    "\n",
    "**This is the notebook you should copy and edit for your own actual analyses**\n",
    "\n",
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5720f96",
   "metadata": {},
   "source": [
    "These are magics that provide certain functionality. Specifically, if you edit functions that are called in this notebook, the functions are reloaded so the changes propagate here without needing to reload the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b857e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903fdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress, ttest_ind, ttest_rel, ttest_1samp\n",
    "import pandas as pd\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "import os \n",
    "import joblib\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import warnings \n",
    "\n",
    "# I only want to see warnings once\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd439a",
   "metadata": {},
   "source": [
    "Note: If you have installed the LFPAnalysis package in editable form on Minerva, you must append the local path! This is because Minerva requires that you point your package installs away from the local directory for space reasons, but editable packages have to be installed locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b3889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/hpc/users/qasims01/resources/LFPAnalysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b549c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LFPAnalysis import lfp_preprocess_utils, sync_utils, analysis_utils, nlx_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0445cfb",
   "metadata": {},
   "source": [
    "## Load, pre-process and re-reference the neural data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9167e",
   "metadata": {},
   "source": [
    "Substitute your own patients in the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36590691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify root directory for un-archived data and results \n",
    "base_dir = '/sc/arion' \n",
    "# Specify subject names \n",
    "subj_ids = ['MS012', 'MS016']\n",
    "subj_sites = ['MSSM', 'MSSM']\n",
    "# Specify the format being loaded. Probably smart to put subjects and their format in a dataframe. \n",
    "subj_formats = ['edf', 'edf']\n",
    "elec_dict = {f'{x}': [] for x in subj_ids}\n",
    "mne_dict = {f'{x}': [] for x in subj_ids}\n",
    "photodiode_dict = {f'{x}': [] for x in subj_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae1c39",
   "metadata": {},
   "source": [
    "## Pre-process (run 1x): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0c76f",
   "metadata": {},
   "source": [
    "In the pre-processing functions below, we: \n",
    "\n",
    "1. load the raw data (either a .edf file or a folder of .nlx files) into mne objects for use with the mne toolbox: https://mne.tools/stable/index.html.\n",
    "\n",
    "2. load the localized electrode names from the .csv or .xlsx file listing their MNI coordinates into the mne object\n",
    "\n",
    "3. filter and resample as necessary\n",
    "\n",
    "4. re-reference to a proximal white matter electrode (or bipolar, if desired) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83f681a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ix, subj_id in enumerate(subj_ids): \n",
    "    site = subj_sites[ix]\n",
    "    format = subj_formats[ix]\n",
    "    \n",
    "    print(f'Working on subj {subj_id}')\n",
    "    \n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    elec_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/anat/'\n",
    "    save_path = f'{base_dir}/projects/guLab/Salman/EphysAnalyses/{subj_id}/neural/Day1'\n",
    "    \n",
    "    # Check if path exists for saving, and if not, make it\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # electrode files could either be csv or excel\n",
    "    elec_files = glob(f'{elec_path}/*.csv') + glob(f'{elec_path}/*.xlsx')\n",
    "    # There should really only be one, so grab it with the zero-index \n",
    "    elec_file = elec_files[0]\n",
    "\n",
    "    # Make MNE file\n",
    "    mne_data = lfp_preprocess_utils.make_mne(load_path=load_path, \n",
    "                                             elec_path=elec_file,\n",
    "                                             format=format,\n",
    "                                             return_data=True,\n",
    "                                             site=site,\n",
    "                                             check_bad=False) # changed this to not annotate anything as bad \n",
    "\n",
    "    # Save this data so that you don't need this step again:\n",
    "    mne_data.save(f'{save_path}/raw_ieeg.fif', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516aae8-a1ed-4c7c-94db-576f30ed99ae",
   "metadata": {},
   "source": [
    "## DEPRECATED: ~Manually annotate bad channels before re-referencing~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155c0a4-9f1b-40e8-8e31-e4fed1e92bda",
   "metadata": {},
   "source": [
    "~Even though the pipeline automatically detects channels with high variance compared to the rest of the data, it can still be very useful to scroll through the data and manually click on channels that are bad (or click on automatically bad channels that are,in fact, not bad).~\n",
    "\n",
    "~Feel free to be liberal with this. Bad channels are not omitted from analysis; they are simply omitted from being used as references, so they don't spread the 'badness' to other channels.~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f2e66",
   "metadata": {},
   "source": [
    "~We want to identify channels that are CLEARLY artifactual. I say CLEARLY because there are some noisy channels that will clear up with re-referencing. We want to pick channels here only if they are clear outliers that should not be USED as references for any other channels - this would simply spread their noise around.~\" \n",
    "\n",
    "Note: Viewing/scrolling/editing these channels requires Jupyter NOTEBOOK - won't work in JupyterLab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72c0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook \n",
    "# # Note: this does not work in JupyterLAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3255787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scroll up/down and left/right using your keyboard. CLICK on a channel to turn it 'grey' and mark as a 'bad' channel. \n",
    "# # If you click a grey channel again it will unmark it. \n",
    "\n",
    "# subj_id = 'MS012'\n",
    "# save_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}/neural/Day1'\n",
    "# mne_data = mne.io.read_raw_fif(f'{save_path}/raw_ieeg.fif', preload=True)\n",
    "# fig = mne_data.plot(start=0, duration=120, n_channels=30, \n",
    "#                       scalings=mne_data._data.max()/30\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae772702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save this manual data\n",
    "# mne_data.save(f'{save_path}/raw_ieeg.fif', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835543d",
   "metadata": {},
   "source": [
    "## Re-reference the data (default=bipolar): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f462f2e-2a28-47c3-bce4-6e064ec4f1c0",
   "metadata": {},
   "source": [
    "We re-reference the data to get rid of shared noise, cleaning the data to leave what we assume is local biological activity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, subj_id in enumerate(subj_ids): \n",
    "    site = subj_sites[ix]\n",
    "    format = subj_formats[ix]\n",
    "\n",
    "    print(f'Working on subj {subj_id}')\n",
    "    \n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    elec_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/anat/'\n",
    "    save_path = f'{base_dir}/projects/guLab/Salman/EphysAnalyses/{subj_id}/neural/Day1'\n",
    "    # Check if path exists for saving, and if not, make it\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # electrode files could either be csv or excel\n",
    "    elec_files = glob(f'{elec_path}/*.csv') + glob(f'{elec_path}/*.xlsx')\n",
    "    # There should really only be one, so grab it with the zero-index \n",
    "    elec_file = elec_files[0]\n",
    "\n",
    "    # Make MNE file\n",
    "    mne_data = mne.io.read_raw_fif(f'{save_path}/raw_ieeg.fif', preload=True)\n",
    "\n",
    "\n",
    "    # Re-reference neural data\n",
    "    mne_data_reref = lfp_preprocess_utils.ref_mne(mne_data=mne_data, \n",
    "                                                  elec_path=elec_file, \n",
    "                                                  method='bipolar', \n",
    "                                                  site=site)\n",
    "\n",
    "    # Save this data so that you don't need this step again:\n",
    "    mne_data_reref.save(f'{save_path}/bp_ref_ieeg.fif', overwrite=True)\n",
    "\n",
    "    # Should also save out re-referenced elec_file: \n",
    "\n",
    "    elec_data = lfp_preprocess_utils.load_elec(elec_file)\n",
    "    anode_list = [x.split('-')[0] for x in mne_data_reref.ch_names]\n",
    "    elec_df = elec_data[elec_data.label.str.lower().isin(anode_list)]\n",
    "    elec_df['label'] =  elec_df.label.apply(lambda x: [a for a in mne_data_reref.ch_names if str(x).lower() in a.split('-')[0]][0])\n",
    "\n",
    "    # add region to the data frame \n",
    "    manual_col = [col for col in elec_df.columns if 'manual' in col.lower()][0]\n",
    "    all_regions = [] \n",
    "    for chan_name in elec_df.label.unique():\n",
    "        elec_region = analysis_utils.select_rois_picks(elec_df, chan_name, manual_col=manual_col)\n",
    "        all_regions.append(elec_region) \n",
    "\n",
    "    elec_df['salman_region'] = all_regions\n",
    "    elec_df['hemisphere'] = elec_df.label.apply(lambda x: x[0])\n",
    "\n",
    "    elec_df.to_csv(f'{base_dir}/projects/guLab/Salman/EphysAnalyses/{subj_id}/reref_elec_df', index=False)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff05564",
   "metadata": {},
   "source": [
    " - mne_data: a Raw mne object, where the data has been loaded, filtered for line noise, searched for bad channels, parsed for different data types, and resampled if necessary. \n",
    " \n",
    " - mne_data_reref: an mne object containing re-referenced data (either white matter or bipolar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b53a3d",
   "metadata": {},
   "source": [
    "## NOW look at the data to manually remove channels: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b12839",
   "metadata": {},
   "source": [
    "After bipolar referencing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll up/down and left/right using your keyboard. CLICK on a channel to turn it 'grey' and mark as a 'bad' channel. \n",
    "# If you click a grey channel again it will unmark it. \n",
    "\n",
    "subj_id = 'MS012'\n",
    "save_path = f'{base_dir}/projects/guLab/Salman/EphysAnalyses/{subj_id}/neural/Day1'\n",
    "mne_data = mne.io.read_raw_fif(f'{save_path}/bp_ref_ieeg.fif', preload=True)\n",
    "fig = mne_data.plot(start=0, duration=120, n_channels=30, \n",
    "                      scalings=mne_data._data.max()/30\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90817f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALSO look at the power spectra! \n",
    "# You can click on channels here to identify them, and go back to the viz above to mark them as noise if need be\n",
    "\n",
    "mne_data_reref.compute_psd().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d310de9",
   "metadata": {},
   "source": [
    "If you have ran the preprocessing above, load the data instead: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b657ca72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_176324/4234602883.py:14: RuntimeWarning: This filename (/sc/arion/projects/guLab/Salman/EMU/MS012/neural/Day1/photodiode.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  photodiode_data = mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True)\n",
      "/tmp/ipykernel_176324/4234602883.py:14: RuntimeWarning: This filename (/sc/arion/projects/guLab/Salman/EMU/MS016/neural/Day1/photodiode.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  photodiode_data = mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True)\n",
      "/tmp/ipykernel_176324/4234602883.py:14: RuntimeWarning: This filename (/sc/arion/projects/guLab/Salman/EMU/MS025/neural/Day1/photodiode.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  photodiode_data = mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True)\n"
     ]
    }
   ],
   "source": [
    "for subj_id in subj_ids: \n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    elec_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/anat/'\n",
    "    save_path = f'{base_dir}/work/qasims01/MemoryBanditData/EMU/Subjects/{subj_id}/neural/Day1'\n",
    "\n",
    "    elec_files = glob(f'{elec_path}/*.csv') + glob(f'{elec_path}/*.xlsx')\n",
    "    # There should really only be one \n",
    "    elec_file = elec_files[0]\n",
    "    elec_data = lfp_preprocess_utils.load_elec(elec_file)\n",
    "\n",
    "    mne_data_reref = mne.io.read_raw_fif(f'{save_path}/bp_ref_ieeg.fif', preload=True)\n",
    "\n",
    "    photodiode_data = mne.io.read_raw_fif(f'{load_path}/photodiode.fif', preload=True)\n",
    "\n",
    "    # Append to list \n",
    "    mne_dict[subj_id].append(mne_data_reref)\n",
    "\n",
    "    photodiode_dict[subj_id].append(photodiode_data)\n",
    "\n",
    "    elec_dict[subj_id].append(elec_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e10c5",
   "metadata": {},
   "source": [
    " - mne_dict: a dictionary containing all of your subjects' re-referenced mne data \n",
    " \n",
    " - photodiode_dict: a dictionary containing all of your subjects' photodiode data \n",
    " \n",
    " - elec_dict: a dictionary containing the paths to your subjects' electrode data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b7377",
   "metadata": {},
   "source": [
    "## Sync behavioral and neural data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57df8f",
   "metadata": {},
   "source": [
    "Here, we perform a critical step: computing the time offset between the computer that recorded the neural data and the laptop that featured the experiment. \n",
    "\n",
    "The function here only requires a **subset** of detected sync signals (i.e. photodiode deflections) to be detected to successfully compute this offset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501a02c-8417-4608-b8ab-dcefe44c53f8",
   "metadata": {},
   "source": [
    "First, you may need to MANUALLY clean the photodiode signal if the recording quality is poor. Load it, plot it, and try to isolate/amplify the pulses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d6565-fd46-43fe-9266-68e7f05c1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_id = 'MS015'\n",
    "temp_diode = photodiode_dict[subj_id][0]._data[0, :]\n",
    "temp_diode[900000:] = np.nanmin(temp_diode)\n",
    "photodiode_dict[subj_id][0]._data = temp_diod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480fb71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 blocks\n",
      "............................\n",
      "\n",
      "found matches for 180 of 428 pulses\n",
      "28 blocks\n",
      "............................\n",
      "\n",
      "found matches for 345 of 426 pulses\n",
      "31 blocks\n",
      "...............................\n",
      "\n",
      "found matches for 300 of 469 pulses\n"
     ]
    }
   ],
   "source": [
    "slopes = {f'{x}': [] for x in subj_ids}\n",
    "offsets = {f'{x}': [] for x in subj_ids}\n",
    "\n",
    "for subj_id in subj_ids: \n",
    "        \n",
    "    # Load the behavioral timestamps: \n",
    "    behav_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/behav/Day1'\n",
    "    temp_df = pd.read_csv(glob(f'{behav_path}/*.csv')[0], index_col=None, header=0)\n",
    "    # Load in the timestamps pertaining to your sync. If your task had a square pop up, for example, grab the times for that square's appearance from the behavioral logs.\n",
    "    # Below, I do this for my own task's Psychopy output, but yours is probably coded differently. \n",
    "    beh_ts = temp_df[temp_df.keys()[temp_df.keys().str.startswith('sync') & temp_df.keys().str.endswith('started')]].values\n",
    "    beh_ts = beh_ts[~np.isnan(beh_ts)] \n",
    "\n",
    "    # Synchronize to the photodiode or whatever your neural sync signal is\n",
    "    height = 1\n",
    "    windSize = 15\n",
    "    smoothSize = 11\n",
    "\n",
    "    slope, offset = sync_utils.synchronize_data(beh_ts, \n",
    "                                                photodiode_dict[subj_id][0], \n",
    "                                                smoothSize=smoothSize, windSize=windSize, height=height)\n",
    "\n",
    "    print(slope)\n",
    "    print(offset)\n",
    "    slopes[subj_id].append(slope)\n",
    "    offsets[subj_id].append(offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcdc1d",
   "metadata": {},
   "source": [
    " - slopes: a dictionary containing the slopes (should be ~ 1) for each subject\n",
    " - offsets: a dictionary containing the offsets for each subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd052688",
   "metadata": {},
   "source": [
    "## Load your behavioral data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1451c",
   "metadata": {},
   "source": [
    "You probably have a separate notebook for processing the behavioral data for your task. Load the processed dataframe here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4de3ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "behav_data = pd.read_csv(f'{base_dir}/work/qasims01/MemoryBanditData/EMU/learn_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter just to the participants in this notebook\n",
    "behav_data = behav_data[behav_data.participant.isin(subj_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9e1bc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>feedback_start</th>\n",
       "      <th>baseline_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MS012</td>\n",
       "      <td>243.239158</td>\n",
       "      <td>244.929025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MS012</td>\n",
       "      <td>248.344187</td>\n",
       "      <td>250.043187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MS012</td>\n",
       "      <td>254.083059</td>\n",
       "      <td>255.790670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MS012</td>\n",
       "      <td>258.148220</td>\n",
       "      <td>259.838892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MS012</td>\n",
       "      <td>261.943712</td>\n",
       "      <td>263.620631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant  feedback_start  baseline_start\n",
       "0       MS012      243.239158      244.929025\n",
       "1       MS012      248.344187      250.043187\n",
       "2       MS012      254.083059      255.790670\n",
       "3       MS012      258.148220      259.838892\n",
       "4       MS012      261.943712      263.620631"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behav_data.head(5)[['participant', 'feedback_start', 'baseline_start']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833f764",
   "metadata": {},
   "source": [
    "## Make epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944dc32",
   "metadata": {},
   "source": [
    "Make epochs and remove IEDs. Currently just doing this for one example period - when subjects receive feedback. \n",
    "\n",
    "Notes: \n",
    "\n",
    "- I also segment a baseline period for every event of interest. \n",
    "\n",
    "- I apply a buffer period of 1.0 seconds - this will be helpful when we compute spectrograms later. \n",
    "\n",
    "- The IED count for every channel is added to the epoch metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65b21a",
   "metadata": {},
   "source": [
    "(I'm a little dumb, so my baseline is a fixation cross AFTER the trial, rather than before. A bit silly if you ask me.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a53422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set some windows of interest \n",
    "\n",
    "buf = 1.0 # this is the buffer before and after that we use to limit edge effects for TFRs\n",
    "\n",
    "IED_args = {'peak_thresh':5,\n",
    "           'closeness_thresh':0.25, \n",
    "           'width_thresh':0.2}\n",
    "\n",
    "# evs = ['gamble_start', 'feedback_start', 'baseline_start']\n",
    "evs = {'gamble_start': [-1.0, 0.5],\n",
    "       'feedback_start': [-0.5, 1.5],\n",
    "       'baseline_start': [0, 0.75]}\n",
    "\n",
    "\n",
    "# add behavioral times of interest \n",
    "for subj_id in subj_ids:\n",
    "    # Set paths\n",
    "    load_path = f'{base_dir}/projects/guLab/Salman/EMU/{subj_id}/neural/Day1'\n",
    "    save_path = f'{base_dir}/projects/guLab/Salman/EphysAnalyses/{subj_id}/neural/Day1'\n",
    "\n",
    "    epochs_all_evs = {f'{x}': np.nan for x in evs}\n",
    "    for event in evs.keys():\n",
    "        pre = evs[event][0]\n",
    "        post = evs[event][1]\n",
    "        fixed_baseline = None\n",
    "        behav_times = learn_df[(learn_df.participant==subj_id)][event]\n",
    "\n",
    "        # THE following function will now SAVE out dataframes that indicate IED and artifact time points in your data\n",
    "        \n",
    "        epochs = lfp_preprocess_utils.make_epochs(load_path=f'{save_path}/bp_ref_ieeg.fif', \n",
    "                                                  slope=slopes[subj_id][day][0], offset=offsets[subj_id][day][0], \n",
    "                                                  behav_name=event, behav_times=behav_times,\n",
    "                                                  ev_start_s=pre, ev_end_s=post, buf_s=1, downsamp_factor=None, IED_args=IED_args, detrend=0)\n",
    "\n",
    "\n",
    "        epochs_all_evs[event] = epochs\n",
    "        epochs_all_evs[event].save(f'{save_path}/{event}-epo.fif', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0abb83a",
   "metadata": {},
   "source": [
    " - epochs_all_evs: dictionary containing the epochs for all of your subjects' re-referenced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7661bf",
   "metadata": {},
   "source": [
    "Plot and examine the epochs if you'd like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af96abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# fig = epochs_all_subjs_all_evs['MS007']['feedback_start'].plot(n_epochs=10, n_channels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6261e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Need this following line to save the annotations to the epochs object \n",
    "# fig.fake_keypress('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af3e03",
   "metadata": {},
   "source": [
    "## Where do I go from here? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5654949-ac77-4f95-b8bd-20451b63b8c6",
   "metadata": {},
   "source": [
    "At this point, you've successfuly pre-processed your iEEG data and sliced it around your timepoints of interest. These epochs are going to be the currency for many of your subsequent analyses, so make sure you TRUST THEM before proceeding to the other notebooks for analyses. \n",
    "\n",
    "From here, you can move on to the:\n",
    "\n",
    "1. FOOOF: a notebook for computing power-spectra across trials and fitting their peaks \n",
    "\n",
    "2. TFRPlotsAndStatistics: a notebook for computing time-frequency spectra (trial-level), and computing several different statistics or simply saving the data out in dataframes. \n",
    "\n",
    "3. OscillationDetection(BOSC): a notebook for computing sliding burst detection and saving the data out in dataframes \n",
    "\n",
    "4. TimeResolvedRegression: a notebook for computing regression analysis at each timepoint of a timeseries. TFR-extracted band power is used as example. \n",
    "\n",
    "5. ConnectivityAnalysis: a notebook for computing different synchrony measures between electrodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace9a41-91c2-4aff-bd14-116c08833abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp_analysis",
   "language": "python",
   "name": "lfpanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
